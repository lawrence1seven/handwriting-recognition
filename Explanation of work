About the Algorithm
The various modules of the proposed system includes Pre-processing, Feature Extraction, MinMaxScaler -fitting of data, Image normalization and Classification. 
A.PRE-PROCESSING : Pre-processing of input image is carried out by converting the given image into gray-scale image. Usually a normal colored image consist of three channels- red channel, green channel, blue channel commonly known as RGB. Then the coloured image is converted it to gray-scale image which consist of single monochrome channel in order to avoid unwanted noise in the image. The given input image would be of varied size which may be lead to loss of accurate prediction when the image is compared with that of trained convolutional neural network. So the image is resized and placed upon a empty 28 x 28 pixel blank image so that the image resolution matches the resolution of EMNIST dataset. 
B.FEATURE EXTRACTION : Feature extraction is the process of transforming the input data into a set of features which can very well represent the input data. Feature extraction is related to dimensionality reduction. When the input data is too large to be processed, then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data.After resizing the image, pixel values are obtained in the form of 1D array which represents values between 255 and 0 based on pixel intensity. 
C.MIN MAX SCALER : The min-max scalar form of normalization uses the mean and standard deviation to box all the data into a range lying between a certain min and max value. It transforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one. This transformation is often used as an alternative to zero mean, unit variance scaling. It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values). The MinMaxScaler is the probably the most famous scaling algorithm, and follows the following formula for each feature: (xi–min(x))/ (max(x)–min(x) ) 
D.IMAGE NORMALIZATION : Normalization is a process that changes the range of pixel intensity values. Normalization is sometimes called contrast stretching or histogram stretching. In this input image the normalization is carried out by removing the background pixels and the character alone will be provided as it is in the image. This can be done by using a random value so that the background pixels will have a value certainly less than the pixel values of shades of the character. In this way the image is normalized such that the image is similar to the values in the EMNIST dataset. In this image, the pixel values are more than 0 for the region where the character ‘A’ is written and all other regions have pixel values 0 after image normalization.
 E. CLASSIFICATION : Convolutional neural network is used as a classifier for classifying the handwritten character from the input image. A CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers. A CNN consists of three major components which are convolutional layer, pooling layer and output layer. The activation function that is commonly used with CNN is ReLU which stands for Rectified Linear Unit. Convolution layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. The pooling layer is a form of nonlinear down-sampling. Max pooling is the most common which partition the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum. ReLU applies the non-saturating activation function . It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer. A rectified linear unit has output 0 if the input is less than 0, and raw output otherwise. Its value is obtained based on the formula which is as follows: f(x)= max(x,0) The softmax function is often used in the final layer of a neural network-based classifier. The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1. The output of the softmax function is equivalent to a categorical probability distribution. Thus, softmax function calculates the probabilities distribution of the event over ‘n’ different events.
